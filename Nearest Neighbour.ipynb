{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71801ec1",
   "metadata": {},
   "source": [
    "# NEAREST NEIGHBOUR \n",
    "\n",
    "###### This notebook provides an implementation of Nearest Neighbours methods for classification:\n",
    "1) K-Nearest Neighbor method (k=1) <br>\n",
    "2) Conformal Predictor. <br> \n",
    "\n",
    "Each algorithm is implemented on two datasets: <br>\n",
    "1) <strong>Iris dataset from scikit-learn</strong>: In this dataset, each sample has 4 features describing sepal length, sepal width, petal length, and petal width of an iris plant. There are three possible labels, Y = {0,1,2} <br><br>\n",
    "2) <strong>Dataset imported from ionosphere.txt</strong>: The file ionosphere.txt contains data collected by a radar system in Goose Bay, Labrador. This system consists of a co-phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. Each line of this file represents one labelled sample with comma-separated features. The last number in the line describes the classication. \\Good\" (+1) radar returns are those showing evidence of some type of structure in the ionosphere. \\Bad\" (-1) returns are those that do not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d51b5",
   "metadata": {},
   "source": [
    "#### Loading Datasets\n",
    "\n",
    "We will begin by loading both our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15764099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris dataset is loaded!\n"
     ]
    }
   ],
   "source": [
    "#Loading Iris Dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print('Iris dataset is loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227608db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ionosphere dataset is loaded!\n"
     ]
    }
   ],
   "source": [
    "#Loading Ionosphere Dataset\n",
    "import numpy as np\n",
    "ionosphere = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\")\n",
    "print('Ionosphere dataset is loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ee030",
   "metadata": {},
   "source": [
    "#### Splitting datasets into training and test sets\n",
    "\n",
    "Now we will split our datasets into training and test sets. When dividing the datasets into training and test sets, we will rely on the default convention in scikit-learn i.e. 75% of dataset will be treated as training and rest 25% as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b1ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Iris dataset into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(iris['data'],iris['target'], random_state=2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845fb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Ionosphere data and labels separately to allow separate splitting\n",
    "ionosphere_data = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\",usecols=np.arange(ionosphere.shape[1]-1))\n",
    "ionosphere_target = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\",usecols=ionosphere.shape[1]-1, dtype='int')\n",
    "\n",
    "#Splitting Ionosphere dataset into train and test sets\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(ionosphere_data,ionosphere_target, random_state=2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d640a",
   "metadata": {},
   "source": [
    "### 1) Implementing the 1 Nearest Neighbours method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26373a35",
   "metadata": {},
   "source": [
    "We will begin by defining some helper functions to compute the norm and euclidean distance of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f90e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to calculate norm\n",
    "def norm(vec):\n",
    "    '''\n",
    "    Input: vector\n",
    "    Output: Norm of the vector\n",
    "    '''\n",
    "    n = 0\n",
    "    for i in range(vec.shape[0]):\n",
    "        prod = vec[i]*vec[i]\n",
    "        n = n + prod\n",
    "    return np.sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a942fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to calculate euclidean distance\n",
    "def euclidean_distance(u,v):\n",
    "    '''\n",
    "    Input: Two vectors\n",
    "    Output: Euclidean distance between two vectors\n",
    "    '''\n",
    "    return norm(u-v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b1dde",
   "metadata": {},
   "source": [
    "Now we will define a method to implement a K Nearest Neighbour algorithm for K=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8819a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function implementing KNN algorithm for N=1 to get predictions\n",
    "import math\n",
    "def getKNNPredictions(X_train, y_train, X_test):\n",
    "    '''\n",
    "    Input: Training data and labels alongwith Test data\n",
    "    Output: Predicted labels for test data\n",
    "    '''\n",
    "    pred_labels = []\n",
    "    for i in range(len(X_test)):\n",
    "        test_sample = X_test[i]\n",
    "        min_ed = math.inf\n",
    "        min_index = math.inf\n",
    "        for j in range(len(X_train)):\n",
    "            train_sample = X_train[j]\n",
    "            ed = euclidean_distance(train_sample,test_sample)\n",
    "            if ed < min_ed:\n",
    "                min_ed = ed\n",
    "                min_index = j\n",
    "        pred_labels.append(y_train[min_index])\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8b294",
   "metadata": {},
   "source": [
    "#### Implementing KNN algorithm \n",
    "\n",
    "Now we will implement the above defined algorithm for 1 Nearest neighbor on both our datasets. This will help us make predictions on our test set with the defined model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d42fa",
   "metadata": {},
   "source": [
    "##### Implementing KNN for Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a403986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have predicted values for test sets of Iris datasets!\n",
      "For instance: Our model predicts that test sample [6.  2.2 4.  1. ] belongs to class 1\n"
     ]
    }
   ],
   "source": [
    "y1_pred = getKNNPredictions(X1_train, y1_train, X1_test)\n",
    "print('We have predicted values for test sets of Iris datasets!')\n",
    "\n",
    "from numpy import random\n",
    "index1 = random.randint(X1_test.shape[0])\n",
    "print('For instance: Our model predicts that test sample',X1_test[index1], 'belongs to class',y1_pred[index1] )\n",
    "\n",
    "#Implementing KNN for Ionosphere Dataset\n",
    "y2_pred = getKNNPredictions(X2_train, y2_train, X2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c8cb9",
   "metadata": {},
   "source": [
    "##### Implementing KNN for Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1da1126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have predicted values for test sets of Iris datasets!\n",
      "For instance: Our model predicts that test sample [ 1.       0.       0.38521  0.15564  0.41245  0.07393  0.26459  0.24125\n",
      "  0.23346  0.1323   0.19455  0.25292  0.24514  0.36965  0.08949  0.22957\n",
      " -0.03891  0.36965  0.05058  0.24903  0.24903  0.09728  0.07782  0.29961\n",
      " -0.02494  0.28482 -0.06024  0.26256 -0.14786  0.14786 -0.09339  0.31128\n",
      " -0.19066  0.28794] belongs to class 1\n"
     ]
    }
   ],
   "source": [
    "y2_pred = getKNNPredictions(X2_train, y2_train, X2_test)\n",
    "print('We have predicted values for test sets of Iris datasets!')\n",
    "\n",
    "index2 = random.randint(X2_test.shape[0])\n",
    "print('For instance: Our model predicts that test sample',X2_test[index2], 'belongs to class',y2_pred[index2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157fc835",
   "metadata": {},
   "source": [
    "#### Evaluating the Model\n",
    "\n",
    "Now we will compute the performance of our KNN algorithm on both our datasets. To understand the performance of our algorithm, we will calculate the number of errors it makes on the test set and the test error rate (the ratio of the number of errors to the size of the test set) along with the percentage of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10486c0b",
   "metadata": {},
   "source": [
    "###### Performance of KNN on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a27d0a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors = 1\n",
      "Test error rate = 0.02631578947368421\n",
      "Percentage of correct predictions = 97.36842105263158 %\n"
     ]
    }
   ],
   "source": [
    "#number of errors it makes on the test set\n",
    "errors = 0\n",
    "for i in range(len(y1_test)):\n",
    "    if y1_pred[i] != y1_test[i]:\n",
    "        errors = errors + 1\n",
    "        \n",
    "print('Number of errors =',errors)\n",
    "\n",
    "#computing the test error rate \n",
    "test_error_rate = errors/len(y1_test)\n",
    "print('Test error rate =',test_error_rate)\n",
    "\n",
    "# Alternatively, \n",
    "# test_error_rate = np.mean(y1_pred != y1_test)\n",
    "# But this method is less efficient computationally \n",
    "\n",
    "#Calculating percentage of correct predictions\n",
    "correct_predictions = len(y1_test)-errors\n",
    "success_percent = correct_predictions/len(y1_test)*100\n",
    "print(\"Percentage of correct predictions =\",success_percent,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9a0e3",
   "metadata": {},
   "source": [
    "###### Performance of KNN on Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b59ef3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors = 11\n",
      "Test error rate = 0.125\n",
      "Percentage of correct predictions= 87.5 %\n"
     ]
    }
   ],
   "source": [
    "#number of errors it makes on the test set\n",
    "errors = 0\n",
    "for i in range(len(y2_test)):\n",
    "    if y2_pred[i] != y2_test[i]:\n",
    "        errors = errors + 1\n",
    "        \n",
    "print('Number of errors =',errors)\n",
    "\n",
    "#computing the test error rate \n",
    "test_error_rate = errors/len(y2_test)\n",
    "print('Test error rate =',test_error_rate)\n",
    "\n",
    "# Alternatively, \n",
    "#test_error_rate = np.mean(y2_pred != y2_test)\n",
    "# But this method is less efficient computationally \n",
    "\n",
    "#Calculating percentage of correct predictions\n",
    "correct_predictions = len(y2_test)-errors\n",
    "success_percent = correct_predictions/len(y2_test)*100\n",
    "print(\"Percentage of correct predictions=\",success_percent,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e75ba9",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "\n",
    "For 1 Nearest algorithm model, the test set accuracy on Iris dataset is about 0.97, which means we made the right predictions for 97% of the irises in the test set. This high level of accuracy means that our model may be trustworthy enough to use.\n",
    "On the other hand, the test set accuracy on Ionosphere dataset is about 0.87, which means we made the right predictions for around 87% of the radars in the test set.\n",
    "With these statistical results, there are lots of caveats, such as: Is our training set large enough to make this kind of conclusions? Or can it be a statistical fluke? We can get partial answers to these questions from our Conformal Predictor (section 2 of the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420160f4",
   "metadata": {},
   "source": [
    "##### Verifying our model with that of sci-kit learn implementation\n",
    "\n",
    "We now have the performance of our KNN algorithm on both the datasets. We can also verify our implementation by matching its performace with that of scikit-learn implementation of KNN neighbors.\n",
    "<I>This is only for verification!</I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a2722af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of scikit-learn KNN algorithm on Iris dataset:\n",
      "Test error rate = 0.02631578947368418\n",
      "Percentage of correct predictions = 97.36842105263158 %\n"
     ]
    }
   ],
   "source": [
    "#Verifying the result for Iris dataset using scikit-learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1.fit(X1_train, y1_train)\n",
    "y1_pred = knn1.predict(X1_test)\n",
    "\n",
    "print(\"Performance of scikit-learn KNN algorithm on Iris dataset:\")\n",
    "test_error_rate = 1-knn1.score(X1_test,y1_test)\n",
    "print('Test error rate =',test_error_rate)\n",
    "success_percent = knn1.score(X1_test,y1_test) * 100\n",
    "print(\"Percentage of correct predictions =\",success_percent,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "057b92b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of scikit-learn KNN algorithm on Ionosphere dataset:\n",
      "Test error rate(scikit-learn)= 0.125\n",
      "Percentage of correct predictions = 87.5 %\n"
     ]
    }
   ],
   "source": [
    "#Verifying using scikit Ionosphere\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn2 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn2.fit(X2_train, y2_train)\n",
    "y2_pred = knn2.predict(X2_test)\n",
    "\n",
    "print(\"Performance of scikit-learn KNN algorithm on Ionosphere dataset:\")\n",
    "test_error_rate = 1-knn2.score(X2_test,y2_test)\n",
    "print('Test error rate(scikit-learn)=',test_error_rate)\n",
    "success_percent = knn2.score(X2_test,y2_test) * 100\n",
    "print(\"Percentage of correct predictions =\",success_percent,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36d79e",
   "metadata": {},
   "source": [
    "##### Verified\n",
    "Since the performance of both the algorithms are same, we can conclude that our KNN algorithm is working as expected! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb30f64",
   "metadata": {},
   "source": [
    "### 2) Implementing the Conformal Predictor\n",
    "We will now implement the conformal predictor that can make predictions with a guranteed probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f20bb",
   "metadata": {},
   "source": [
    "We will begin by defining some helper functions to compute rank of a number in a sequence and a function to handle divisons as per standard definitions for special cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcfca084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to find rank of conformity score of test sample in a sequence of conformity scores of extended training set\n",
    "#Intuitive idea: Rank of a number is k when it is the kth smallest element\n",
    "def rank(seq):\n",
    "    '''\n",
    "    Input: Sequence of conformity scores of samples of extended traning set\n",
    "    Output: Rank of conformity score of test sample\n",
    "    '''\n",
    "    score_of_interest = seq[-1]\n",
    "    count = 0\n",
    "    for n in range(len(seq)):\n",
    "        if score_of_interest >= seq[n]:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fbdea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heler function to get a/b considering as per the standard definitions\n",
    "def div(a,b):\n",
    "    '''\n",
    "    Input: Two operands for division\n",
    "    Output: This functions returns 0/0 as 0 and n/0 as infinity, otherwise performs normal division on operands\n",
    "    '''\n",
    "    if b == 0:\n",
    "        if a == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.inf\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a77b76",
   "metadata": {},
   "source": [
    "We will implement our conformal predictor based on the Nearest Neighbour conformity measure = the distance to the nearest sample of a different class/ the distance to the nearest sample of the same class. Below we will define functions to compute p-values and average false p-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e00aba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function that returns the corresponding p-value for augmented set created with a p-label\n",
    "def pvalue(augmented_train_x, augmented_train_y):\n",
    "    '''\n",
    "    Input: Extended labelled training set with p-label for test sample\n",
    "    Output: p-value for postulated label\n",
    "    '''\n",
    "    conformal_scores = []\n",
    "    for i,sample in enumerate(augmented_train_x):\n",
    "        same_class_nearest_distance = math.inf\n",
    "        diff_class_nearest_distance = math.inf\n",
    "        conformal_score = 0\n",
    "        for j,potential_neighbor in enumerate(augmented_train_x):\n",
    "            #we need to skip the case where sample itself is considered as its own potential nearest neighbor\n",
    "            if j != i:\n",
    "                #potential neighbor is a same class neighbor\n",
    "                if augmented_train_y[i] == augmented_train_y[j]:   \n",
    "                    same_class_neighbor_distance = euclidean_distance(sample,potential_neighbor)\n",
    "                    if same_class_nearest_distance > same_class_neighbor_distance:\n",
    "                        same_class_nearest_distance = same_class_neighbor_distance\n",
    "                #potential neighbor is a different class neighbor\n",
    "                else:\n",
    "                    diff_class_neighbor_distance = euclidean_distance(sample,potential_neighbor)\n",
    "                    if diff_class_nearest_distance > diff_class_neighbor_distance:\n",
    "                        diff_class_nearest_distance = diff_class_neighbor_distance\n",
    "        conformal_score = div(diff_class_nearest_distance,same_class_nearest_distance)\n",
    "        conformal_scores.append(conformal_score)\n",
    "\n",
    "    #Calculating the rank based the on conformal scores of extended training set\n",
    "    rank_test_sample = rank(conformal_scores)\n",
    "    p_value = rank_test_sample/len(augmented_train_x)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57b167a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to calculate average false p-values\n",
    "def avgfalsepvalue(p_matrix, y_test, label_space):\n",
    "    sum_false_p_value = 0\n",
    "    count_false_p_value = 0\n",
    "    #For each test sample\n",
    "    for i in range(len(p_matrix)):\n",
    "        true_label = y_test[i]\n",
    "        true_label_index = label_space.index(true_label)\n",
    "        for j in range(len(p_matrix[i])):\n",
    "            #condition to check if it is a false label or not\n",
    "            if j != true_label_index:\n",
    "                false_p_value = p_matrix[i][j]\n",
    "                sum_false_p_value += false_p_value\n",
    "                count_false_p_value += 1\n",
    "    avg_false_p_value = sum_false_p_value/count_false_p_value\n",
    "    return avg_false_p_value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5856e4",
   "metadata": {},
   "source": [
    "##### Implementing Conformal Predictor for Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80ac33d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated p-values for all postulated labels of every test sample in our iris dataset!\n"
     ]
    }
   ],
   "source": [
    "iris_p_matrix = []\n",
    "for i in range(len(X1_test)):\n",
    "    p_values = []\n",
    "    test_sample = np.array([X1_test[i]])\n",
    "    iris_augmented_x = np.concatenate((X1_train, test_sample))\n",
    "    iris_label_space = list(set(iris['target']))\n",
    "    for p_label in iris_label_space:\n",
    "        y1_test_new = np.array([p_label])\n",
    "        iris_augmented_y = np.concatenate((y1_train,y1_test_new))\n",
    "        p_value = pvalue(iris_augmented_x, iris_augmented_y)\n",
    "        p_values.append(p_value)\n",
    "    iris_p_matrix.append(p_values)\n",
    "        \n",
    "print('Calculated p-values for all postulated labels of every test sample in our iris dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9670d1",
   "metadata": {},
   "source": [
    "We have stored our predictions, i.e. p-values for all labels for every test sample, in a p-matrix. \n",
    "This matrix has records for all test samples so number of rows should be equal to test set of iris dataset and number of columns equal to the label space. Each entry represent a p-value for that test sample(row) with a p-label represented by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0fd1591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in p_matrix = 38 which is equivalent to test set of size 38\n",
      "Columns in p_matrix = 3 which is equivalent to label space of size [0, 1, 2]\n",
      "\n",
      "For instance:\n",
      "iris_p_matrix[ 27 ][ 0 ]= gives p-value for iris test sample 27 with a postulated label at index 0 of label space\n"
     ]
    }
   ],
   "source": [
    "#Defining structure of p-matrix for iris\n",
    "p_row_size = len(iris_p_matrix)\n",
    "print('Rows in p_matrix =',p_row_size,end=\" \")\n",
    "print('which is equivalent to test set of size',X1_test.shape[0])\n",
    "\n",
    "p_col_size = len(iris_p_matrix[0])\n",
    "print('Columns in p_matrix =',p_col_size, end=\" \")\n",
    "print('which is equivalent to label space of size',iris_label_space)\n",
    "\n",
    "#Defining a random entry of p-matrix\n",
    "print(\"\\nFor instance:\")\n",
    "row = random.randint(p_row_size)\n",
    "col = random.randint(p_col_size)\n",
    "print('iris_p_matrix[',row,'][',col,']=','gives p-value for iris test sample',row,'with a postulated label at index',col,'of label space')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e82f8",
   "metadata": {},
   "source": [
    "##### Implementing Conformal Predictor for Ionosphere dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61f71f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated p-values for all postulated labels of every test sample in our ionosphere dataset!\n"
     ]
    }
   ],
   "source": [
    "ionosphere_p_matrix = []\n",
    "for i in range(len(X2_test)):\n",
    "    p_values = []\n",
    "    test_sample = np.array([X2_test[i]])\n",
    "    ionosphere_augmented_x = np.concatenate((X2_train, test_sample))\n",
    "    ionosphere_label_space = list(set(ionosphere_target))\n",
    "    for p_label in ionosphere_label_space:\n",
    "        y2_test_new = np.array([p_label])\n",
    "        ionosphere_augmented_y = np.concatenate((y2_train,y2_test_new))\n",
    "        p_value = pvalue(ionosphere_augmented_x, ionosphere_augmented_y )\n",
    "        p_values.append(p_value)\n",
    "    ionosphere_p_matrix.append(p_values)\n",
    "    \n",
    "print('Calculated p-values for all postulated labels of every test sample in our ionosphere dataset!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "689fc5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in p_matrix = 88 which is equivalent to test set of size 88\n",
      "Columns in p_matrix = 2 which is equivalent to label space of size [1, -1]\n",
      "\n",
      "For instance:\n",
      "ionosphere_p_matrix[ 9 ][ 1 ]= gives p-value for ionosphere test sample 9 with a postulated label at index 1 of label space\n"
     ]
    }
   ],
   "source": [
    "#Defining structure of p-matrix for ionosphere\n",
    "p_row_size = len(ionosphere_p_matrix)\n",
    "print('Rows in p_matrix =',p_row_size,end=\" \")\n",
    "print('which is equivalent to test set of size',X2_test.shape[0])\n",
    "\n",
    "p_col_size = len(ionosphere_p_matrix[0])\n",
    "print('Columns in p_matrix =',p_col_size, end=\" \")\n",
    "print('which is equivalent to label space of size',ionosphere_label_space)\n",
    "\n",
    "#Defining a random entry of p-matrix\n",
    "print(\"\\nFor instance:\")\n",
    "from numpy import random\n",
    "row = random.randint(p_row_size)\n",
    "col = random.randint(p_col_size)\n",
    "print('ionosphere_p_matrix[',row,'][',col,']=','gives p-value for ionosphere test sample',row,'with a postulated label at index',col,'of label space')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9ff99",
   "metadata": {},
   "source": [
    "#### Evaluating the Model\n",
    "\n",
    "We will now measure the efficiency of our conformal predictor by calculating the average false p-value for both the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d7c98",
   "metadata": {},
   "source": [
    "##### Calculating average false p-value for Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7ef666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average false p-value = 0.011527713088029821\n"
     ]
    }
   ],
   "source": [
    "#With all computed p-values for all test samples, compute the average false p-value\n",
    "iris_result = avgfalsepvalue(iris_p_matrix, y1_test, iris_label_space)\n",
    "print('Average false p-value =',iris_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcda5f",
   "metadata": {},
   "source": [
    "##### Calculating average false p-value for Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6705ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average false p-value = 0.0585829889807163\n"
     ]
    }
   ],
   "source": [
    "#With all computed p-values for all test samples, compute the average false p-value\n",
    "ionosphere_result = avgfalsepvalue(ionosphere_p_matrix, y2_test, ionosphere_label_space )\n",
    "print('Average false p-value =',ionosphere_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c37120",
   "metadata": {},
   "source": [
    "##### End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
